{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q-4ZLr6A3_Yy"
      },
      "source": [
        "# Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration\n",
        "This Colab noteook contains the code for reproducing the results from the publication \"Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration\" by Daniel Deutsch, George Foster, and Markus Freitag.\n",
        "\n",
        "If you just want to use tie calibration and pairwise accuracy as proposed in the paper, here is an example of the most direct way of doing so:\n",
        "\n",
        "```python\n",
        "from mt_metrics_eval import tau_optimization\n",
        "from mt_metrics_eval import stats\n",
        "\n",
        "# M is the number of groups, N is the number of observations per group.\n",
        "# For instance, in the case of the group-by-item correlations, M is the number\n",
        "# of items (or segments) and N is the number of systems. If you have no\n",
        "# groups, M=1.\n",
        "M = 10\n",
        "N = 20\n",
        "\n",
        "# Generate fake data for this example. X should be the matrix of metric scores\n",
        "# and Y should be the matrix of human scores\n",
        "X = np.random.rand(M, N)\n",
        "Y = np.random.rand(M, N)\n",
        "\n",
        "# Run tie calibration (called tau_optimization in this notebook). The sample_rate\n",
        "# parameter indicates what proportion of all possible pairs of observations\n",
        "# to use when searching for the optimal epsilon. If 1.0, all will be used. If\n",
        "# you have a very large number of pairs, you may want to lower this. We found\n",
        "# that small values (0.1) actually tend to yield relatively stable results\n",
        "# (see Appendix E in the paper).\n",
        "sample_rate = 1.0\n",
        "result = tau_optimization.tau_optimization(\n",
        "  X, Y, tau_optimization.TauSufficientStats.acc_23,\n",
        ")\n",
        "\n",
        "# The result object has various information, including `best_threshold` (equal\n",
        "# to the best epsilon), `best_tau` (equal to the best `acc_23` score), plus\n",
        "# `thresholds` and `taus` (they contain the various epsilsons and corresponding\n",
        "# accuracy scores that were used in the search).\n",
        "print(result.best_threshold)\n",
        "print(result.best_tau)\n",
        "\n",
        "# If you already have an epsilon and you want to compute an accuracy score\n",
        "# with that epsilon with two vectors, then you can do so with the following:\n",
        "x = np.random.rand(N)  # the metric scores\n",
        "y = np.random.rand(N)  # the human scores\n",
        "epsilon = 0.05\n",
        "accuracy, _ = stats.KendallVariants(\n",
        "  Y, X, variant=\"acc23\", epsilon=epsilon\n",
        ")\n",
        "print(accuracy)\n",
        "```\n",
        "\n",
        "If you use this meta-evaluation methodology, please cite the following paper:\n",
        "```\n",
        "@misc{deutsch2023ties,\n",
        "      title={{Ties Matter: Meta-Evaluating Modern Metrics with Pairwise Accuracy and Tie Calibration}},\n",
        "      author={Daniel Deutsch and George Foster and Markus Freitag},\n",
        "      year={2023},\n",
        "      eprint={2305.14324},\n",
        "      archivePrefix={arXiv},\n",
        "      primaryClass={cs.CL}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "696rbZ11E6-l"
      },
      "source": [
        "## Environment Setup\n",
        "Installs the MTME library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ql2EuOTD3sfv"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/google-research/mt-metrics-eval.git \u0026\u0026 cd mt-metrics-eval \u0026\u0026 git checkout d18c3ebe91a004c124c179ad5614b8dba96f1f48 \u0026\u0026 pip install ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0msK-QWFGUU"
      },
      "source": [
        "## Download Data\n",
        "Downloads the WMT'22 metrics scores and the GEMBA metric outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP5btM7746c9"
      },
      "outputs": [],
      "source": [
        "# MTME data\n",
        "!python3 -m mt_metrics_eval.mtme --download\n",
        "\n",
        "# GEMBA data\n",
        "!mkdir -p gemba/wmt22/metric-scores/en-de gemba/wmt22/metric-scores/en-ru gemba/wmt22/metric-scores/zh-en\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-de/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-ru/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score -O gemba/wmt22/metric-scores/zh-en/GEMBA-Dav3-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-de/GEMBA-GPT4-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/en-ru/GEMBA-GPT4-DA-refA.seg.score\n",
        "!wget https://raw.githubusercontent.com/MicrosoftTranslator/GEMBA/main/mt-metrics-eval-v2/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score -O gemba/wmt22/metric-scores/zh-en/GEMBA-GPT4-DA-refA.seg.score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o-PgbRNFNM9"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7wlbEG839na"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "from mt_metrics_eval import data as mtme_data\n",
        "from mt_metrics_eval import stats as mtme_stats\n",
        "from mt_metrics_eval import tau_optimization\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from typing import Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMJAyHmSFOvb"
      },
      "source": [
        "## Load WMT'22 Evaluation Sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJmSbDP44gUv"
      },
      "outputs": [],
      "source": [
        "eval_sets = {\n",
        "    \"en-de\": mtme_data.EvalSet(\"wmt22\", \"en-de\", read_stored_metric_scores=True, path=[\"/root/.mt-metrics-eval/mt-metrics-eval-v2\", \"/content/gemba\"]),\n",
        "    \"en-ru\": mtme_data.EvalSet(\"wmt22\", \"en-ru\", read_stored_metric_scores=True, path=[\"/root/.mt-metrics-eval/mt-metrics-eval-v2\", \"/content/gemba\"]),\n",
        "    \"zh-en\": mtme_data.EvalSet(\"wmt22\", \"zh-en\", read_stored_metric_scores=True, path=[\"/root/.mt-metrics-eval/mt-metrics-eval-v2\", \"/content/gemba\"]),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjlymCXAFUQy"
      },
      "source": [
        "## Utility Functions\n",
        "Implements getting and filtering scores, calculating correlations that aren't checked in to the MTME library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6TdWE4F6wRo"
      },
      "outputs": [],
      "source": [
        "def get_metric_scores(evs: mtme_data.EvalSet, metric: str) -\u003e dict[str, list[float]]:\n",
        "  scores_dict = evs.Scores(\"seg\", metric)\n",
        "  bad_systems = evs.outlier_sys_names | {evs.std_ref}\n",
        "  return {\n",
        "      system: scores for system, scores in scores_dict.items() if system not in bad_systems\n",
        "  }\n",
        "\n",
        "\n",
        "def kendall(x, y, variant):\n",
        "  x = np.asarray(x).ravel()\n",
        "  y = np.asarray(y).ravel()\n",
        "\n",
        "  if x.size != y.size:\n",
        "    raise ValueError(\n",
        "        'All inputs to `kendalltau` must be of the same '\n",
        "        f'size, found x-size {x.size} and y-size {y.size}'\n",
        "    )\n",
        "  elif not x.size or not y.size:\n",
        "    raise ValueError('x or y are empty')\n",
        "\n",
        "  # check both x and y\n",
        "  cnx = np.any(np.isnan(x))\n",
        "  cny = np.any(np.isnan(y))\n",
        "  contains_nan = cnx or cny\n",
        "  if contains_nan:\n",
        "    raise ValueError('x or y contains NaN')\n",
        "\n",
        "  def count_rank_tie(ranks):\n",
        "    cnt = np.bincount(ranks).astype('int64', copy=False)\n",
        "    cnt = cnt[cnt \u003e 1]\n",
        "    return (\n",
        "        (cnt * (cnt - 1) // 2).sum(),\n",
        "        (cnt * (cnt - 1.0) * (cnt - 2)).sum(),\n",
        "        (cnt * (cnt - 1.0) * (2 * cnt + 5)).sum(),\n",
        "    )\n",
        "\n",
        "  size = x.size\n",
        "  perm = np.argsort(y)  # sort on y and convert y to dense ranks\n",
        "  x, y = x[perm], y[perm]\n",
        "  y = np.r_[True, y[1:] != y[:-1]].cumsum(dtype=np.intp)\n",
        "\n",
        "  # stable sort on x and convert x to dense ranks\n",
        "  perm = np.argsort(x, kind='mergesort')\n",
        "  x, y = x[perm], y[perm]\n",
        "  x = np.r_[True, x[1:] != x[:-1]].cumsum(dtype=np.intp)\n",
        "\n",
        "  dis = scipy.stats._stats._kendall_dis(x, y)  # discordant pairs\n",
        "\n",
        "  obs = np.r_[True, (x[1:] != x[:-1]) | (y[1:] != y[:-1]), True]\n",
        "  cnt = np.diff(np.nonzero(obs)[0]).astype('int64', copy=False)\n",
        "\n",
        "  ntie = (cnt * (cnt - 1) // 2).sum()  # joint ties\n",
        "  xtie, _, _ = count_rank_tie(x)  # ties in x, stats\n",
        "  ytie, _, _ = count_rank_tie(y)  # ties in y, stats\n",
        "\n",
        "  tot = (size * (size - 1)) // 2\n",
        "  con = tot - ((xtie - ntie) + (ytie - ntie) + ntie + dis)\n",
        "\n",
        "  minclasses = min(len(set(x)), len(set(y)))\n",
        "\n",
        "  tx = xtie - ntie\n",
        "  ty = ytie - ntie\n",
        "  txy = ntie\n",
        "\n",
        "  if variant == \"a\":\n",
        "    return (con - dis) / (con + dis + tx + ty + txy), 0\n",
        "  elif variant == \"b\":\n",
        "    return (con - dis) / np.sqrt((con + dis + tx) * (con + dis + ty)), 0\n",
        "  elif variant == \"c\":\n",
        "    minclasses = min(len(set(x)), len(set(y)))\n",
        "    return 2 * (con - dis) / (size**2 * (minclasses - 1) / minclasses), 0\n",
        "  elif variant == \"10\":\n",
        "    return (con - dis - ty) / (con + dis + ty), 0\n",
        "  elif variant == \"13\":\n",
        "    return (con - dis) / (con + dis), 0\n",
        "  elif variant == \"14\":\n",
        "    return (con - dis) / (con + dis + ty), 0\n",
        "  elif variant == \"acc_eq\":\n",
        "    # Accuracy assuming tie optimization is done.\n",
        "    return (con + txy) / (con + dis + tx + ty + txy), 0\n",
        "  return 0\n",
        "\n",
        "\n",
        "def custom_kendall(corr, variant: str, average_by: str = \"none\"):\n",
        "  cf = corr.AverageCorrelation(\n",
        "      kendall, average_by, variant=variant)\n",
        "  return cf(corr.gold_scores, corr.metric_scores)\n",
        "\n",
        "\n",
        "def calculate_correlations(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    mqm_scores: dict[str, list[float]],\n",
        "    metric_scores: dict[str, list[float]],\n",
        "    coef: str,\n",
        ") -\u003e dict[str, float]:\n",
        "  corr = evs.Correlation(mqm_scores, metric_scores)\n",
        "  if coef == \"kendall-a\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"a\")\n",
        "  elif coef == \"kendall-b\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"b\")\n",
        "  elif coef == \"kendall-c\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"c\")\n",
        "  elif coef == \"kendall-10\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"10\")\n",
        "  elif coef == \"kendall-13\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"13\")\n",
        "  elif coef == \"kendall-14\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"14\")\n",
        "  elif coef == \"accuracy-eq_no_calib\":\n",
        "    corr_fn = functools.partial(custom_kendall, corr=corr, variant=\"acc_eq\")\n",
        "  elif coef == \"accuracy-eq\":\n",
        "    corr_fn = functools.partial(\n",
        "        corr.KendallWithTiesOpt,\n",
        "        sample_rate=1.0\n",
        "    )\n",
        "  elif coef == \"pearson\":\n",
        "    corr_fn = corr.Pearson\n",
        "  else:\n",
        "    raise ValueError(coef)\n",
        "\n",
        "  no_grouping = corr_fn()[0]\n",
        "  group_by_item, _, num_items = corr_fn(average_by=\"item\")\n",
        "  group_by_system = corr_fn(average_by=\"sys\")[0]\n",
        "  return {\n",
        "      \"no_grouping\": no_grouping,\n",
        "      \"group_by_item\": group_by_item,\n",
        "      \"group_by_item_num_items\": num_items,\n",
        "      \"group_by_system\": group_by_system,\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pR69U_pFeCl"
      },
      "source": [
        "## Analyze Ties\n",
        "These results correspond to Tables 3, 4, and 10 from the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9icczg96GzO"
      },
      "outputs": [],
      "source": [
        "def analyze_ties(grouping: str, metric: str) -\u003e pd.DataFrame:\n",
        "  df = []\n",
        "  for lp, evs in eval_sets.items():\n",
        "    mqm_dict = get_metric_scores(evs, \"mqm\")\n",
        "    scores_dict = get_metric_scores(evs, metric)\n",
        "    num_translations = 0\n",
        "    num_pairs = 0\n",
        "    num_tied_pairs = 0\n",
        "    num_zero_pairs = 0\n",
        "\n",
        "    if grouping == \"no_grouping\":\n",
        "      all_scores = []\n",
        "      for system, scores in scores_dict.items():\n",
        "        for i, score in enumerate(scores):\n",
        "          if score is not None and mqm_dict[system][i] is not None:\n",
        "            all_scores.append(score)\n",
        "\n",
        "      num_translations = len(all_scores)\n",
        "      for i in range(len(all_scores)):\n",
        "        for j in range(i + 1, len(all_scores)):\n",
        "          num_pairs += 1\n",
        "          if all_scores[i] == all_scores[j]:\n",
        "            num_tied_pairs += 1\n",
        "            if all_scores[i] == 0.0:\n",
        "              num_zero_pairs += 1\n",
        "\n",
        "    elif grouping == \"group_by_item\":\n",
        "      for i in range(len(evs.src)):\n",
        "        item_scores = []\n",
        "        for system, scores in scores_dict.items():\n",
        "          if scores[i] is not None and mqm_dict[system][i] is not None:\n",
        "            item_scores.append(scores[i])\n",
        "\n",
        "        num_translations += len(item_scores)\n",
        "        for j in range(len(item_scores)):\n",
        "          for k in range(j + 1, len(item_scores)):\n",
        "            num_pairs += 1\n",
        "            if item_scores[j] == item_scores[k]:\n",
        "              num_tied_pairs += 1\n",
        "              if item_scores[j] == 0.0:\n",
        "                num_zero_pairs += 1\n",
        "\n",
        "    elif grouping == \"group_by_system\":\n",
        "      for system, scores in scores_dict.items():\n",
        "        system_scores = []\n",
        "        for i, score in enumerate(scores):\n",
        "          if score is not None and mqm_dict[system][i] is not None:\n",
        "            system_scores.append(score)\n",
        "\n",
        "        num_translations += len(system_scores)\n",
        "        for i in range(len(system_scores)):\n",
        "          for j in range(i + 1, len(system_scores)):\n",
        "            num_pairs += 1\n",
        "            if system_scores[i] == system_scores[j]:\n",
        "              num_tied_pairs += 1\n",
        "              if system_scores[i] == 0.0:\n",
        "                num_zero_pairs += 1\n",
        "\n",
        "    df.append({\n",
        "        \"lp\": lp,\n",
        "        \"num_translations\": num_translations,\n",
        "        \"num_pairs\": num_pairs,\n",
        "        \"num_tied_pairs\": num_tied_pairs,\n",
        "        \"percent_of_pairs_tied\": num_tied_pairs / num_pairs * 100,\n",
        "        \"num_zero_pairs\": num_zero_pairs,\n",
        "        \"percent_of_tied_pairs_zero_tied\": num_zero_pairs / num_tied_pairs * 100,\n",
        "        \"percent_of_pairs_zero_tied\": num_zero_pairs / num_pairs * 100,\n",
        "    })\n",
        "  return pd.DataFrame(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hFLAvOvFh6Q"
      },
      "source": [
        "### MQM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZH5TK1NE8mYl"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"no_grouping\", \"mqm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wrjUYMR7fEu"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"mqm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQMmc2-F7gjh"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_system\", \"mqm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcI9o5g-Fl4Q"
      },
      "source": [
        "### Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lB7QrD09P67"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"metricx_xxl_MQM_2020-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp3N_rr--vJM"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"COMET-22-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLu--Vx--v-5"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"MATESE-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtbV8xsA-2G-"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"GEMBA-Dav3-DA-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJgQzf2w-2tr"
      },
      "outputs": [],
      "source": [
        "analyze_ties(\"group_by_item\", \"GEMBA-GPT4-DA-refA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXspo9BMFonV"
      },
      "source": [
        "## Equal Width Buckets Experiments\n",
        "This experiment maps a metric score into k buckets of each width, simulating what would happen if the metric predicted a larger number of ties than it actually does.\n",
        "This experiment produces Figure 3."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nju81PpA0xy"
      },
      "outputs": [],
      "source": [
        "def map_to_equal_width_buckets(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    scores_dict: dict[str, list[float]],\n",
        "    num_buckets: int,\n",
        ") -\u003e dict[str, list[float]]:\n",
        "  \"\"\"Maps the scores to integer buckets where each bucket represents an equal score range.\"\"\"\n",
        "  all_scores = [x for scores in scores_dict.values() for x in scores]\n",
        "  min_value = min(all_scores)\n",
        "  max_value = max(all_scores)\n",
        "  width = (max_value - min_value) / num_buckets\n",
        "  bins = [min_value + width * (i + 1) for i in range(num_buckets - 1)]\n",
        "  return {\n",
        "      system: np.digitize(scores, bins)  for system, scores in scores_dict.items()\n",
        "  }\n",
        "\n",
        "def run_equal_width_buckets_experiment(lp: str, metric: str, coef: str):\n",
        "  evs = eval_sets[lp]\n",
        "  num_buckets_list = [\n",
        "      2, 3, 4, 5, 10, 25, 50, 100, 200, 500, 1000, 5000, 10000, 20000, 30000\n",
        "  ]\n",
        "  mqm_dict = get_metric_scores(evs, \"mqm\")\n",
        "  scores_dict = get_metric_scores(evs, metric)\n",
        "\n",
        "  # Calculate the correlation when the scores are bucketed\n",
        "  correlations = []\n",
        "  num_non_nan_segments = []\n",
        "  for num_buckets in num_buckets_list:\n",
        "    bucketed_scores = map_to_equal_width_buckets(evs, scores_dict, num_buckets)\n",
        "    correlations_dict = calculate_correlations(evs, mqm_dict, bucketed_scores, coef)\n",
        "    correlations.append(correlations_dict[\"group_by_item\"])\n",
        "    num_non_nan_segments.append(correlations_dict[\"group_by_item_num_items\"])\n",
        "\n",
        "  # Calculate the original correlation\n",
        "  original_dict = calculate_correlations(evs, mqm_dict, scores_dict, coef)\n",
        "  original = original_dict[\"group_by_item\"]\n",
        "\n",
        "  return {\n",
        "      \"num_buckets\": num_buckets_list,\n",
        "      \"bucketed_correlations\": correlations,\n",
        "      \"num_segments\": num_non_nan_segments,\n",
        "      \"original_correlation\": original,\n",
        "      \"original_correlation_num_segments\": original_dict[\"group_by_item_num_items\"],\n",
        "  }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kokqulivml8T"
      },
      "outputs": [],
      "source": [
        "def plot_bucketed_correlations(\n",
        "    lp: str,\n",
        "    metric: str,\n",
        "    data: dict[str, Any],\n",
        "):\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 8))\n",
        "\n",
        "  num_buckets = data[\"num_buckets\"]\n",
        "  x = np.log10(num_buckets)\n",
        "\n",
        "  y1_bucketed = data[\"bucketed_correlations\"]\n",
        "  y1_original = data[\"original_correlation\"]\n",
        "\n",
        "  y2_bucketed = data[\"num_segments\"]\n",
        "  y2_original = data[\"original_correlation_num_segments\"]\n",
        "\n",
        "  axes[0].plot(x, y1_bucketed, label=\"Bucketed Scores\", color=\"blue\", marker=\"o\", markersize=10)\n",
        "  axes[0].axhline(y1_original, label=\"Original Scores\", linestyle=\"dashed\", color=\"orange\")\n",
        "\n",
        "  axes[1].plot(x, y2_bucketed, label=\"Bucketed Scores\", color=\"blue\", marker=\"o\", markersize=10)\n",
        "  axes[1].axhline(y2_original, label=\"Original Scores\", linestyle=\"dashed\", color=\"orange\")\n",
        "\n",
        "  axes[0].set_ylabel(\"Group-by-Item $\\\\tau_b$\")\n",
        "  axes[1].set_xlabel(\"log$_{10}$(Number of Buckets)\")\n",
        "  axes[1].set_ylabel(\"#Non-NaN Groups\")\n",
        "\n",
        "  handles, labels = axes[0].get_legend_handles_labels()\n",
        "\n",
        "  # Add legend with the modified handles and labels. Disable the legend box,\n",
        "  # which only takes up space and doesn't look good anyway.\n",
        "  # Reshape the line symbol a bit.\n",
        "  axes[0].legend(handles, labels, frameon=False, handlelength=2.2,\n",
        "                 handletextpad=0.5, borderpad=1)\n",
        "\n",
        "  # Modify space beween subplots\n",
        "  fig.subplots_adjust(hspace=0.2)\n",
        "\n",
        "  fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnYaFY86nIZ_"
      },
      "outputs": [],
      "source": [
        "lp = \"en-de\"\n",
        "metric = \"metricx_xxl_MQM_2020-refA\"\n",
        "results = run_equal_width_buckets_experiment(lp, metric, \"kendall-b\")\n",
        "plot_bucketed_correlations(lp, metric, results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLuJp9OoF8vH"
      },
      "source": [
        "## Tie Calibration Experiments\n",
        "This reproduces the main result of the paper.\n",
        "It ranks metrics by different correlation coefficients, including the pairwise accuracy with tie calibration.\n",
        "These results correspond to Tables 6 and 12-20."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umlX7yCdperh"
      },
      "outputs": [],
      "source": [
        "def select_optimal_epsilon(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    metric: str,\n",
        "    grouping: str,\n",
        "):\n",
        "  mqm_scores = get_metric_scores(evs, \"mqm\")\n",
        "  metric_scores = get_metric_scores(evs, metric)\n",
        "  if not metric_scores:\n",
        "    return None\n",
        "\n",
        "  if grouping == \"no_grouping\":\n",
        "    average_by = \"none\"\n",
        "    sample_rate = 0.1\n",
        "  elif grouping == \"group_by_item\":\n",
        "    average_by = \"item\"\n",
        "    sample_rate = 1.0\n",
        "  elif grouping == \"group_by_system\":\n",
        "    average_by = \"sys\"\n",
        "    sample_rate = 0.1\n",
        "  else:\n",
        "    raise ValueError(grouping)\n",
        "\n",
        "  corr = evs.Correlation(mqm_scores, metric_scores)\n",
        "\n",
        "  return mtme_stats.KendallWithTiesOpt(\n",
        "      corr.gold_scores,\n",
        "      corr.metric_scores,\n",
        "      num_sys=corr.num_sys,\n",
        "      average_by=average_by,\n",
        "      sample_rate=sample_rate,\n",
        "  )\n",
        "\n",
        "\n",
        "def run_tie_calibration_experiment(\n",
        "    evs: mtme_data.EvalSet,\n",
        "    grouping: str,\n",
        "    metrics: list[str],\n",
        "):\n",
        "  mqm_scores = get_metric_scores(evs, \"mqm\")\n",
        "  coefs = [\"kendall-a\", \"kendall-b\", \"kendall-c\", \"kendall-10\", \"kendall-13\", \"kendall-14\", \"accuracy-eq_no_calib\"]\n",
        "\n",
        "  df = []\n",
        "  for metric in metrics:\n",
        "    # Run the optimization\n",
        "    opt_results = select_optimal_epsilon(\n",
        "        evs, metric, grouping,\n",
        "    )\n",
        "    if not opt_results:\n",
        "      continue\n",
        "\n",
        "    # Calculate baseline metric scores\n",
        "    metric_scores = get_metric_scores(evs, metric)\n",
        "    correlations = {\n",
        "        coef: calculate_correlations(\n",
        "            evs,\n",
        "            mqm_scores,\n",
        "            metric_scores,\n",
        "            coef,\n",
        "        ) for coef in coefs\n",
        "    }\n",
        "\n",
        "    results = {\n",
        "        \"metric\": metric,\n",
        "        \"best_epsilon\": opt_results[1],\n",
        "        \"best_acc\": opt_results[0],\n",
        "    }\n",
        "    for coef in coefs:\n",
        "      results[coef] = correlations[coef][grouping]\n",
        "    df.append(results)\n",
        "\n",
        "  df = pd.DataFrame(df)\n",
        "  for coef in [\"best_acc\"] + coefs:\n",
        "    df[f\"{coef}_rank\"] = df[coef].rank(ascending=False)\n",
        "\n",
        "  return df.sort_values(by=[\"best_acc_rank\"])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fl5Qb4WgEukK"
      },
      "outputs": [],
      "source": [
        "ende_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"SEScore-refA\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTZYtaEDD4FT"
      },
      "outputs": [],
      "source": [
        "# Too slow to run on the free Colab server\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-de\"],\n",
        "#     \"no_grouping\",\n",
        "#     ende_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFr6nDDlzD_T"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"en-de\"],\n",
        "    \"group_by_item\",\n",
        "    ende_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4-w-vjOD5av"
      },
      "outputs": [],
      "source": [
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-de\"],\n",
        "#     \"group_by_system\",\n",
        "#     ende_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H3y_k_7rpWh"
      },
      "outputs": [],
      "source": [
        "enru_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udGSQw01r3q3"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-ru\"],\n",
        "#     \"no_grouping\",\n",
        "#     enru_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQDGZJTUrWFM"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"en-ru\"],\n",
        "    \"group_by_item\",\n",
        "    enru_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVnFKLxrr4Xm"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"en-ru\"],\n",
        "#     \"group_by_system\",\n",
        "#     enru_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEqF230Lro8_"
      },
      "outputs": [],
      "source": [
        "zhen_metrics = [\n",
        "    \"metricx_xxl_MQM_2020-refA\",\n",
        "    \"UniTE-ref-refA\",\n",
        "    \"COMET-22-refA\",\n",
        "    \"MATESE-refA\",\n",
        "    \"UniTE-src-src\",\n",
        "    \"GEMBA-GPT4-DA-refA\",\n",
        "    \"MATESE-QE-src\",\n",
        "    \"COMETKiwi-src\",\n",
        "    \"MS-COMET-22-refA\",\n",
        "    \"COMET-QE-src\",\n",
        "    \"SEScore-refA\",\n",
        "    \"HWTSC-Teacher-Sim-src\",\n",
        "    \"GEMBA-Dav3-DA-refA\",\n",
        "    \"MEE-refA\",\n",
        "    \"REUSE-src\",\n",
        "    \"BLEURT-20-refA\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrM6nfU5rwEV"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"zh-en\"],\n",
        "#     \"no_grouping\",\n",
        "#     zhen_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3xvVZt--rWBO"
      },
      "outputs": [],
      "source": [
        "run_tie_calibration_experiment(\n",
        "    eval_sets[\"zh-en\"],\n",
        "    \"group_by_item\",\n",
        "    zhen_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_spZUfBirwix"
      },
      "outputs": [],
      "source": [
        "# Too slow/too much memory for public Colab\n",
        "# run_tie_calibration_experiment(\n",
        "#     eval_sets[\"zh-en\"],\n",
        "#     \"group_by_system\",\n",
        "#     zhen_metrics,\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPId69h0GkdT"
      },
      "source": [
        "## Analyze Epsilon Across Years\n",
        "This experiment analyzes whether or not the epsilon selected from one year of WMT generalizes to other years.\n",
        "This corresponds to Figure 4."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH4fHVTm-pIc"
      },
      "outputs": [],
      "source": [
        "def convert_to_matrices(mqm_scores: dict[str, list[float]], metric_scores: dict[str, list[float]]):\n",
        "  X, Y = [], []\n",
        "  for system in mqm_scores.keys():\n",
        "    if system not in metric_scores:\n",
        "      continue\n",
        "    x = metric_scores[system]\n",
        "    y = mqm_scores[system]\n",
        "    if not y or not any(score is not None for score in y):\n",
        "      continue\n",
        "    assert len(x) == len(y)\n",
        "    X.append([x[i] for i in range(len(x)) if y[i] is not None])\n",
        "    Y.append([y[i] for i in range(len(y)) if y[i] is not None])\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def calculate_group_by_item_acc(X: np.ndarray, Y: np.ndarray, epsilon: float):\n",
        "  accs = []\n",
        "  for x, y in zip(X.T, Y.T):\n",
        "    accs.append(mtme_stats.KendallVariants(x, y, variant=\"acc23\", epsilon=epsilon)[0])\n",
        "  return np.mean(accs)\n",
        "\n",
        "\n",
        "def _compare_epsilson_across_years(lp: str, metric: str, ax, legend: bool = True, xlabel = True, ylabel = True):\n",
        "  if lp == \"en-de\":\n",
        "    if metric == \"bleurt\":\n",
        "      metric21 = \"bleurt-20-refC\"\n",
        "      metric22 = \"BLEURT-20-refA\"\n",
        "    elif metric == \"comet\":\n",
        "      metric21 = \"COMET-DA_2020-refC\"\n",
        "      metric22 = \"COMET-20-refA\"\n",
        "    elif metric == \"bleu\":\n",
        "      metric21 = \"sentBLEU-refC\"\n",
        "      metric22 = \"BLEU-refA\"\n",
        "    elif metric == \"bertscore\":\n",
        "      metric21 = \"BERTScore-refC\"\n",
        "      metric22 = \"BERTScore-refA\"\n",
        "  elif lp == \"zh-en\":\n",
        "    if metric == \"bleurt\":\n",
        "      metric21 = \"bleurt-20-refB\"\n",
        "      metric22 = \"BLEURT-20-refA\"\n",
        "    elif metric == \"comet\":\n",
        "      metric21 = \"COMET-DA_2020-refB\"\n",
        "      metric22 = \"COMET-20-refA\"\n",
        "    elif metric == \"bleu\":\n",
        "      metric21 = \"sentBLEU-refB\"\n",
        "      metric22 = \"BLEU-refA\"\n",
        "    elif metric == \"bertscore\":\n",
        "      metric21 = \"BERTScore-refB\"\n",
        "      metric22 = \"BERTScore-refA\"\n",
        "\n",
        "  evs21 = mtme_data.EvalSet(\"wmt21.news\", lp, read_stored_metric_scores=True)\n",
        "  evs22 = mtme_data.EvalSet(\"wmt22\", lp, read_stored_metric_scores=True)\n",
        "\n",
        "  mqm_scores_21 = get_metric_scores(evs21, \"mqm\")\n",
        "  mqm_scores_22 = get_metric_scores(evs22, \"mqm\")\n",
        "  metric_scores_21 = get_metric_scores(evs21, metric21)\n",
        "  metric_scores_22 = get_metric_scores(evs22, metric22)\n",
        "\n",
        "  if lp == \"en-de\":\n",
        "    del mqm_scores_21[\"refB\"]\n",
        "    del metric_scores_21[\"refB\"]\n",
        "\n",
        "  X_21, Y_21 = convert_to_matrices(mqm_scores_21, metric_scores_21)\n",
        "  X_22, Y_22 = convert_to_matrices(mqm_scores_22, metric_scores_22)\n",
        "\n",
        "  res21 = tau_optimization.tau_optimization(\n",
        "      X_21.T, Y_21.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "  res22 = tau_optimization.tau_optimization(\n",
        "      X_22.T, Y_22.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "\n",
        "  acc21_threshold22 = calculate_group_by_item_acc(X_21, Y_21, res22.best_threshold)\n",
        "  acc22_threshold21 = calculate_group_by_item_acc(X_22, Y_22, res21.best_threshold)\n",
        "\n",
        "  print(metric)\n",
        "  print(f\"WMT'21 eps={res21.best_threshold}, WMT'21 acc={res21.best_tau}, WMT'22 acc={acc22_threshold21}\")\n",
        "  print(f\"WMT'22 eps={res22.best_threshold}, WMT'22 acc={res22.best_tau}, WMT'21 acc={acc21_threshold22}\")\n",
        "  print(f\"WMT'21 accuracy abs delta: {(res21.best_tau - acc21_threshold22) * 100}\")\n",
        "  print(f\"WMT'22 accuracy abs delta: {(res22.best_tau - acc22_threshold21) * 100}\")\n",
        "  print(f\"WMT'21 accuracy rel delta: {(res21.best_tau - acc21_threshold22) / res21.best_tau:.2%}\")\n",
        "  print(f\"WMT'22 accuracy rel delta: {(res22.best_tau - acc22_threshold21) / res22.best_tau:.2%}\")\n",
        "  print(f\"WMT'21 -\u003e 22 epsilon abs delta: {res21.best_threshold - res22.best_threshold}\")\n",
        "  print(f\"WMT'22 -\u003e 21 epsilon abs delta: {res22.best_threshold - res21.best_threshold}\")\n",
        "  print(f\"WMT'21 -\u003e 22 epsilon rel delta: {(res21.best_threshold - res22.best_threshold) / res21.best_threshold:.2%}\")\n",
        "  print(f\"WMT'22 -\u003e 21 epsilon rel delta: {(res22.best_threshold - res21.best_threshold) / res22.best_threshold:.2%}\")\n",
        "\n",
        "\n",
        "  ax.plot(res21.thresholds, res21.taus, label=\"WMT'21\", color=\"blue\")\n",
        "  ax.plot(res22.thresholds, res22.taus, label=\"WMT'22\", color=\"orange\")\n",
        "  ax.axvline(res21.best_threshold, color=\"blue\", linestyle=\"dashed\")\n",
        "  ax.axvline(res22.best_threshold, color=\"orange\", linestyle=\"dashed\")\n",
        "  if ylabel:\n",
        "    ax.set_ylabel(\"Pairwise Accuracy\")\n",
        "  if xlabel:\n",
        "    ax.set_xlabel(\"Epsilon\")\n",
        "  ax.set_title(lp)\n",
        "\n",
        "  elements = [\n",
        "      Patch(facecolor=\"blue\", edgecolor=\"blue\", label=\"WMT'21\"),\n",
        "      Patch(facecolor=\"orange\", edgecolor=\"orange\", label=\"WMT'22\"),\n",
        "      Line2D([0], [0], lw=4, color=\"black\", label=\"Accuracy\"),\n",
        "      Line2D([0], [0], lw=4, color=\"black\", label=\"Best Epsilon\", linestyle=\"dashed\"),\n",
        "  ]\n",
        "  if legend:\n",
        "    ax.legend(handles=elements, frameon=False, handlelength=2.2, handletextpad=0.5, borderpad=0.2)\n",
        "\n",
        "\n",
        "def compare_epsilons_across_years(metric: str):\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(10, 6), sharex=True)\n",
        "  _compare_epsilson_across_years(\"en-de\", metric, axes[0], xlabel=False, ylabel=False)\n",
        "  _compare_epsilson_across_years(\"zh-en\", metric, axes[1], legend=False, ylabel=False)\n",
        "  # Modify space beween subplots\n",
        "  fig.subplots_adjust(hspace=0.2)\n",
        "  # plt.xlim(0, 0.4)\n",
        "\n",
        "  fig.text(0.04, 0.5, 'Pairwise Accuracy', va='center', rotation='vertical')\n",
        "\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEWF8K_eHAGY"
      },
      "outputs": [],
      "source": [
        "compare_epsilons_across_years(\"bleurt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scKYKSSqL6yW"
      },
      "source": [
        "## Ties-F1 Experiment\n",
        "This experiment analyzes what happens if you decompose the accuracy score into F1 scores instead.\n",
        "It corresponds to Figure 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCyouarxJ5iF"
      },
      "outputs": [],
      "source": [
        "def _ties_precision(ss) -\u003e float:\n",
        "  \"\"\"Calculates the precision of metric tie predictions.\"\"\"\n",
        "  denom = ss.ties_both + ss.ties_metric\n",
        "  return ss.ties_both / denom if denom \u003e 0 else 1.0\n",
        "\n",
        "def _ties_recall(ss) -\u003e float:\n",
        "  \"\"\"Calculates the recall of human ties.\"\"\"\n",
        "  denom = ss.ties_both + ss.ties_human\n",
        "  return ss.ties_both / denom if denom \u003e 0 else 1.0\n",
        "\n",
        "def _ties_f1(ss) -\u003e float:\n",
        "  precision = _ties_precision(ss)\n",
        "  recall = _ties_recall(ss)\n",
        "  denom = precision + recall\n",
        "  return 2 * (precision * recall) / denom if denom \u003e 0 else 0.0\n",
        "\n",
        "def _correct_rank_precision(ss) -\u003e float:\n",
        "  denom = ss.con + ss.dis + ss.ties_human\n",
        "  return ss.con / denom if denom \u003e 0 else 1.0\n",
        "\n",
        "def _correct_rank_recall(ss) -\u003e float:\n",
        "  denom = ss.con + ss.dis + ss.ties_metric\n",
        "  return ss.con / denom if denom \u003e 0 else 1.0\n",
        "\n",
        "def _correct_rank_f1(ss) -\u003e float:\n",
        "  \"\"\"Calculates the correct rank F1.\"\"\"\n",
        "  precision = _correct_rank_precision(ss)\n",
        "  recall = _correct_rank_recall(ss)\n",
        "  denom = precision + recall\n",
        "  return 2 * (precision * recall) / denom if denom \u003e 0 else 0.0\n",
        "\n",
        "\n",
        "def plot_other_f1s(lp: str, metric: str):\n",
        "  mqm_scores = get_metric_scores(eval_sets[lp], \"mqm\")\n",
        "  metric_scores = get_metric_scores(eval_sets[lp], metric)\n",
        "\n",
        "  X, Y = convert_to_matrices(mqm_scores, metric_scores)\n",
        "\n",
        "  res = tau_optimization.tau_optimization(\n",
        "      X.T, Y.T, tau_optimization.TauSufficientStats.acc_23\n",
        "  )\n",
        "  thresholds = []\n",
        "  taus_subset = []\n",
        "  ties_p_list, ties_r_list, ties_f1_list = [], [], []\n",
        "  rank_p_list, rank_r_list, rank_f1_list = [], [], []\n",
        "  for i in range(0, len(res.thresholds), 1000):\n",
        "    threshold = res.thresholds[i]\n",
        "    taus_subset.append(res.taus[i])\n",
        "    thresholds.append(threshold)\n",
        "\n",
        "    ties_p, ties_r, ties_f1 = [], [], []\n",
        "    rank_p, rank_r, rank_f1 = [], [], []\n",
        "\n",
        "    for x, y in zip(X.T, Y.T):\n",
        "      con, dis, t_x, t_y, t_xy = mtme_stats._MatrixSufficientStatistics(x, y, threshold, None, None)\n",
        "      ss = tau_optimization.TauSufficientStats(con, dis, t_y, t_x, t_xy)\n",
        "      ties_p.append(_ties_precision(ss))\n",
        "      ties_r.append(_ties_recall(ss))\n",
        "      ties_f1.append(_ties_f1(ss))\n",
        "      rank_p.append(_correct_rank_precision(ss))\n",
        "      rank_r.append(_correct_rank_recall(ss))\n",
        "      rank_f1.append(_correct_rank_f1(ss))\n",
        "\n",
        "    ties_p_list.append(np.mean(ties_p))\n",
        "    ties_r_list.append(np.mean(ties_r))\n",
        "    ties_f1_list.append(np.mean(ties_f1))\n",
        "    rank_p_list.append(np.mean(rank_p))\n",
        "    rank_r_list.append(np.mean(rank_r))\n",
        "    rank_f1_list.append(np.mean(rank_f1))\n",
        "\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 4.5))\n",
        "  axes = [ax]\n",
        "  plt.plot(thresholds, taus_subset, label=\"Accuracy\", color=\"blue\")\n",
        "  plt.plot(thresholds, ties_f1_list, label=\"Ties F1\", color=\"orange\")\n",
        "  plt.plot(thresholds, rank_f1_list, label=\"Correct Rank F1\", color=\"green\")\n",
        "  plt.axvline(res.best_threshold, color=\"blue\", linestyle=\"dashed\", label=\"Best Epsilon\")\n",
        "  plt.xlabel(\"Epsilon\")\n",
        "  plt.xlim(-0.1, 2.0)\n",
        "\n",
        "\n",
        "  handles, labels = axes[0].get_legend_handles_labels()\n",
        "\n",
        "  # Add legend with the modified handles and labels. Disable the legend box,\n",
        "  # which only takes up space and doesn't look good anyway.\n",
        "  # Reshape the line symbol a bit.\n",
        "  axes[0].legend(handles, labels, frameon=False, handlelength=2.2,\n",
        "                 handletextpad=0.5, borderpad=1)\n",
        "\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1uUKM_NNzmG"
      },
      "outputs": [],
      "source": [
        "plot_other_f1s(\"en-de\", \"COMET-22-refA\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xH8Jaazv-Y_q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
