{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUPVve909iM8"
      },
      "source": [
        "# Demo colab for mt_metrics_eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "QfP6OuW4aORE"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "\n",
        "import numpy as np\n",
        "import scipy.stats\n",
        "from mt_metrics_eval import meta_info\n",
        "from mt_metrics_eval import data\n",
        "from mt_metrics_eval import stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "code",
        "id": "V3_V7UVqaXmr"
      },
      "outputs": [],
      "source": [
        "# Load all available EvalSets (takes about 40s)\n",
        "\n",
        "all_evs = {}  # name/lp -\u003e evs\n",
        "for testset in meta_info.DATA:\n",
        "  for lp in meta_info.DATA[testset]:\n",
        "    evs = data.EvalSet(testset, lp, True)\n",
        "    all_evs[f'{testset}/{lp}'] = evs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAs-_EOGoOhF"
      },
      "outputs": [],
      "source": [
        "# Print stats for each EvalSet\n",
        "\n",
        "print(f'{\"name\":\u003c20}  segs sys metrics gold  refs std')\n",
        "for name, evs in all_evs.items():\n",
        "  nsegs = len(evs.src)\n",
        "  nsys = len(evs.sys_names)\n",
        "  nmetrics = len(evs.metric_basenames)\n",
        "  gold = evs.StdHumanScoreName('sys')\n",
        "  nrefs = len(evs.ref_names)\n",
        "  std_ref = evs.std_ref\n",
        "\n",
        "  print(f'{name:\u003c20} {nsegs:5d} {nsys:3d} {nmetrics:7d} '\n",
        "        f'{gold:5} {nrefs:4d} {std_ref}') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFUOvH_BEMsC"
      },
      "outputs": [],
      "source": [
        "# Example: sys-level Pearson MQM correlations and significance matrix for\n",
        "# wmt21.news en-de, human outputs included in scoring, primary metric \n",
        "# submissions only. Takes about 20s due to bootstrapping for significance tests.\n",
        "\n",
        "# First step is to compile a map from metric-name -\u003e 'Correlation' objects \n",
        "# containing sufficient stats.\n",
        "evs = all_evs['wmt21.news/en-de']\n",
        "level = 'sys'\n",
        "corrs = data.GetCorrelations(\n",
        "    evs=evs,\n",
        "    level=level,\n",
        "    main_refs={evs.std_ref},\n",
        "    close_refs={'refB'},\n",
        "    include_human=True,\n",
        "    include_outliers=False,\n",
        "    gold_name=evs.StdHumanScoreName(level),\n",
        "    primary_metrics=True)\n",
        "\n",
        "# Compute and print Pearson correlations. \n",
        "pearsons = {m: corr.Pearson()[0] for m, corr in corrs.items()}\n",
        "pearsons = dict(sorted(pearsons.items(), key=lambda x: -x[1]))\n",
        "print('System-level +HT Pearson correlations for wmt21.news en-de:') \n",
        "for m in pearsons:\n",
        "  print(f'{m:\u003c21} {pearsons[m]: f}')\n",
        "print()\n",
        "\n",
        "# Compute and print signficance matrix.\n",
        "ranked_metrics = list(pearsons)\n",
        "n = len(ranked_metrics)\n",
        "sig_matrix = np.zeros((n, n))\n",
        "for i in range(n):\n",
        "  corr1 = corrs[ranked_metrics[i]]\n",
        "  pearson_fcn = corr1.GenCorrFunction(scipy.stats.pearsonr, averaged=False)\n",
        "  for j in range(i + 1, n):\n",
        "    corr2 = corrs[ranked_metrics[j]]\n",
        "    sig_matrix[i, j] = stats.PermutationSigDiff(corr2, corr1, pearson_fcn)\n",
        "\n",
        "print('Significant differences in Pearson correlation:')\n",
        "for i in range(n):\n",
        "  better = ['\u003e' if sig_matrix[i, j] \u003c 0.05 else '=' for j in range(i + 1, n)]\n",
        "  print(f'{ranked_metrics[i]:\u003c22} {\" \".join(better)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dECWMWebgBRY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/public/tools/ml_python:ml_notebook",
        "kind": "private"
      },
      "name": "mt-metrics-eval.ipynb",
      "private_outputs": true,
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/py/mt_metrics_eval/mt_metrics_eval.ipynb?workspaceId=fosterg:metrics::citc",
          "timestamp": 1652908277809
        },
        {
          "file_id": "1MQZfWmkpDz2YKV27oI6E0e-V4gE-ThPc",
          "timestamp": 1652907338392
        }
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
